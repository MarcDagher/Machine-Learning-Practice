{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Dataset\n",
    "import pandas as pd\n",
    "dataframe = pd.read_csv(\"Emotions.csv\")\n",
    "# dataframe = dataframe.drop( range(20000, len(dataframe)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of phrases: 62612\n",
      "Corpus: 26651 phrases\n"
     ]
    }
   ],
   "source": [
    "### Basic Text Preprocessing\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(f\"Original number of phrases: {len(dataframe)}\")\n",
    "\n",
    "stopwords = stopwords.words(\"english\")\n",
    "corpus = []\n",
    "duplicates = set()\n",
    "\n",
    "for i in range(len(dataframe)):\n",
    "\n",
    "  # Lower case and Remove non-letters\n",
    "  text = re.sub(\"[^a-zA-Z]\", \" \", dataframe.loc[i, \"text\"]).lower().split()\n",
    "  \n",
    "  # Lemmatization and Stemming\n",
    "  stemmer = PorterStemmer()\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  text = [stemmer.stem(lemmatizer.lemmatize(word)) for word in text if word not in stopwords]\n",
    "  text = \" \".join(text)\n",
    "  \n",
    "  # Remove duplicate phrases\n",
    "  if text in duplicates:\n",
    "    dataframe = dataframe.drop(i)\n",
    "  else:\n",
    "    # Add text to corpus\n",
    "    corpus.append(text)\n",
    "\n",
    "  duplicates.add(text)  \n",
    "\n",
    "print(f\"Corpus: {len(corpus)} phrases\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 26651\n",
      "Label: 26651\n"
     ]
    }
   ],
   "source": [
    "### Create Bag of Words and its dataframe\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "features = vectorizer.fit_transform( corpus ).toarray()\n",
    "label = dataframe['label']\n",
    "label = label.reset_index( drop = True )\n",
    "\n",
    "wordset = vectorizer.get_feature_names_out()\n",
    "dataframe_BoW = pd.DataFrame(features, columns = wordset)\n",
    "\n",
    "print(f\"Corpus: {len(corpus)}\\nLabel: {len(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create dictionary of sorted {Word: Frequency}\n",
    "dict_BoW = {}\n",
    "\n",
    "for word in dataframe_BoW.columns:\n",
    "  total = sum(dataframe_BoW[word])\n",
    "  dict_BoW[word] = total\n",
    "\n",
    "dict_BoW = dict(reversed(sorted( dict_BoW.items(), key = lambda x: x[1] )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words to Remove: 18051 out of 19140\n"
     ]
    }
   ],
   "source": [
    "### Save words with frequency less than 40\n",
    "words_to_remove = set() \n",
    "for word, count in dict_BoW.items():\n",
    "  if (count < 40):\n",
    "    words_to_remove.add(word)\n",
    "print(f\"Words to Remove: {len(words_to_remove)} out of {len(features[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: 26555\n",
      "Label: 26555\n"
     ]
    }
   ],
   "source": [
    "### Remove the unfrequent words from the corpus\n",
    "\n",
    "indicies_to_remove = [] # To later remove them from the label and the corpus\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "  phrase = corpus[i]\n",
    "  new_phrase = []\n",
    "\n",
    "  # Keep the words that are not in the not_frequent list  \n",
    "  for word in phrase.split():\n",
    "    if not (word in words_to_remove):\n",
    "      new_phrase.append(word)\n",
    "  \n",
    "  new_phrase = \" \".join(new_phrase)\n",
    "  # If the phrase is empty (all the words in the phrase are not frequent), save the phrase's index to remove it and remove its label \n",
    "  if new_phrase == \"\":\n",
    "    indicies_to_remove.append(i)\n",
    "  \n",
    "  # Replace the current phrase in the corpus with the new_phrase\n",
    "  corpus[i] = new_phrase\n",
    "\n",
    "\n",
    "# Remove the empty phrases and their label\n",
    "for index in indicies_to_remove:\n",
    "  label = label.drop( index )\n",
    "  corpus.remove( \"\" )\n",
    "\n",
    "print(f\"Corpus: {len(corpus)}\\nLabel: {len(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrases in features: 26555\n",
      "Words in corpus: 1089\n",
      "Labels: 26555\n"
     ]
    }
   ],
   "source": [
    "### Create new Bag of Words with new features\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "features = vectorizer.fit_transform( corpus ).toarray()\n",
    "\n",
    "print(f\"Phrases in features: {len(features)}\\nWords in corpus: {len(features[0])}\\nLabels: {len(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: 21244, y_train: 21244, X_test: 5311, y_test: 5311\n",
      "Length of element in X_train: 1089\n"
     ]
    }
   ],
   "source": [
    "### Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( features, label.values, test_size = 0.2 )\n",
    "\n",
    "print(f\"X_train: {len(X_train)}, y_train: {len(y_train)}, X_test: {len(X_test)}, y_test: {len(y_test)}\")\n",
    "print(f\"Words in X_train: {len(X_train[0])}\")\n",
    "\n",
    "### Prepare evaluation imports\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "[[ 592   60   65   10   72    0]\n",
      " [  59  768   85    6   60   16]\n",
      " [  41   50 1421   49   73   11]\n",
      " [  12    6  100  189   18    0]\n",
      " [  57   58   84    8 1179    6]\n",
      " [   1   32   21    1   10   91]]\n",
      "\n",
      "Accuracy: 0.8\n",
      "\n",
      "\n",
      "Score: [0.79670588 0.80376471 0.79482353 0.80141176 0.7933145  0.80508475\n",
      " 0.79708098 0.77824859 0.80037665 0.80320151]\n",
      "\n",
      "Mean Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "### Build Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LR = LogisticRegression( max_iter = 10000 )\n",
    "LR.fit( X_train, y_train )\n",
    "LR_prediction = LR.predict( X_test )\n",
    "\n",
    "LR_matrix = confusion_matrix( y_test, LR_prediction )\n",
    "LR_accuracy = accuracy_score( y_test, LR_prediction )\n",
    "print(f\"Logistic Regression\\n{LR_matrix}\\n\\nAccuracy: {LR_accuracy:.2}\\n\")\n",
    "\n",
    "LR_score = cross_val_score(estimator = LR, y = y_train, X = X_train, cv = 10 )\n",
    "print(f\"\\nScore: {LR_score}\\n\\nMean Score: {LR_score.mean():.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "[[ 594   58   72   16   58    1]\n",
      " [  62  752   83    5   64   28]\n",
      " [ 138  112 1120  105  149   21]\n",
      " [  11    9   90  196   18    1]\n",
      " [  79   82   91   21 1106   13]\n",
      " [   0   16   13    2    8  117]]\n",
      "\n",
      "Accuracy: 0.73\n",
      "\n",
      "Score: [0.72564706 0.73129412 0.72988235 0.72705882 0.72269303 0.72834275\n",
      " 0.73258004 0.71798493 0.72175141 0.72975518]\n",
      "\n",
      "Mean Score: 0.73\n"
     ]
    }
   ],
   "source": [
    "### Build Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "TREE = DecisionTreeClassifier(  )\n",
    "TREE.fit( X_train, y_train )\n",
    "TREE_prediction = TREE.predict( X_test )\n",
    "\n",
    "TREE_matrix = confusion_matrix( y_test, TREE_prediction )\n",
    "TREE_accuracy = accuracy_score( y_test, TREE_prediction )\n",
    "TREE_score = cross_val_score(estimator = TREE, y = y_train, X = X_train, cv = 10 )\n",
    "print(f\"Decision Tree\\n{TREE_matrix}\\n\\nAccuracy: {TREE_accuracy:.2}\\n\\nScore: {TREE_score}\\n\\nMean Score: {TREE_score.mean():.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "[[ 616   46   66   14   57    0]\n",
      " [  54  787   60    5   58   30]\n",
      " [  66   57 1322   76  103   21]\n",
      " [  11    7   84  210   12    1]\n",
      " [  59   63   83   23 1155    9]\n",
      " [   2   17   10    1    4  122]]\n",
      "\n",
      "Accuracy: 0.79\n",
      "\n",
      "\n",
      "Score: [0.79670588 0.8        0.78823529 0.80611765 0.79425612 0.79896422\n",
      " 0.80178908 0.78060264 0.79519774 0.79755179]\n",
      "\n",
      "Mean Score: 0.8\n"
     ]
    }
   ],
   "source": [
    "### Build Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "FOREST = RandomForestClassifier( n_estimators = 100)\n",
    "FOREST.fit( X_train, y_train )\n",
    "FOREST_prediction = FOREST.predict( X_test )\n",
    "\n",
    "FOREST_matrix = confusion_matrix( y_test, FOREST_prediction )\n",
    "FOREST_accuracy = accuracy_score( y_test, FOREST_prediction )\n",
    "print(f\"Random Forest\\n{FOREST_matrix}\\n\\nAccuracy: {FOREST_accuracy:.2}\\n\")\n",
    "\n",
    "FOREST_score = cross_val_score(estimator = FOREST, y = y_train, X = X_train, cv = 10 )\n",
    "print(f\"\\nScore: {FOREST_score}\\n\\nMean Score: {FOREST_score.mean():.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes\n",
      "[[301  41   9 228  17 203]\n",
      " [ 28 406  12 200  15 333]\n",
      " [ 43  62 455 606  15 464]\n",
      " [  8   7   9 205   6  90]\n",
      " [ 69 110  14 318 482 399]\n",
      " [  4  12   8  34   3  95]]\n",
      "\n",
      "Accuracy: 0.37\n",
      "\n",
      "\n",
      "Score: [0.35435294 0.38258824 0.38917647 0.36564706 0.37146893 0.37288136\n",
      " 0.36016949 0.35499058 0.37947269 0.34934087]\n",
      "\n",
      "Mean Score: 0.37\n"
     ]
    }
   ],
   "source": [
    "### Build Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "NB = GaussianNB()\n",
    "NB.fit( X_train, y_train )\n",
    "NB_prediction = NB.predict( X_test )\n",
    "\n",
    "NB_matrix = confusion_matrix( y_test, NB_prediction )\n",
    "NB_accuracy = accuracy_score( y_test, NB_prediction )\n",
    "print(f\"Naive Bayes\\n{NB_matrix}\\n\\nAccuracy: {NB_accuracy:.2}\\n\")\n",
    "\n",
    "NB_score = cross_val_score(estimator = NB, y = y_train, X = X_train, cv = 10 )\n",
    "print(f\"\\nScore: {NB_score}\\n\\nMean Score: {NB_score.mean():.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-Nearest Neighbors\n",
      "[[ 516   65  126   28   64    0]\n",
      " [ 174  553  150   19   91    7]\n",
      " [ 199  183 1055   67  129   12]\n",
      " [  33   30  127  104   31    0]\n",
      " [ 211  149  330   44  654    4]\n",
      " [  13   31   42   10   22   38]]\n",
      "\n",
      "Accuracy: 0.55\n",
      "\n",
      "\n",
      "Score: [0.55623529 0.55482353 0.552      0.57694118 0.56167608 0.56450094\n",
      " 0.5480226  0.5579096  0.56497175 0.58145009]\n",
      "\n",
      "Mean Score: 0.56\n"
     ]
    }
   ],
   "source": [
    "### Build K-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "KNN = KNeighborsClassifier( n_neighbors = 5 )\n",
    "KNN.fit( X_train, y_train )\n",
    "KNN_prediction = KNN.predict( X_test )\n",
    "\n",
    "KNN_matrix = confusion_matrix( y_test, KNN_prediction )\n",
    "KNN_accuracy = accuracy_score( y_test, KNN_prediction )\n",
    "print(f\"K-Nearest Neighbors\\n{KNN_matrix}\\n\\nAccuracy: {KNN_accuracy:.2}\\n\")\n",
    "\n",
    "KNN_score = cross_val_score(estimator = KNN, y = y_train, X = X_train, cv = 10 )\n",
    "print(f\"\\nScore: {KNN_score}\\n\\nMean Score: {KNN_score.mean():.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "[[ 572   47   88    8   83    1]\n",
      " [  57  770   96    5   65    1]\n",
      " [  48   52 1431   28   74   12]\n",
      " [   4    4  143  144   30    0]\n",
      " [  50   52  100   11 1174    5]\n",
      " [   1   36   30    0   12   77]]\n",
      "\n",
      "Accuracy: 0.78\n",
      "\n",
      "Score: [0.78776471 0.79858824 0.77270588 0.792      0.77448211 0.78766478\n",
      " 0.78672316 0.7740113  0.78201507 0.78531073]\n",
      "\n",
      "Mean Score: 0.78\n"
     ]
    }
   ],
   "source": [
    "### Build Support Vector Machine\n",
    "from sklearn.svm import SVC\n",
    "SVC = SVC( kernel = \"rbf\" )\n",
    "SVC.fit( X_train, y_train )\n",
    "SVC_prediction = SVC.predict( X_test )\n",
    "\n",
    "SVC_matrix = confusion_matrix( y_test, SVC_prediction )\n",
    "SVC_accuracy = accuracy_score( y_test, SVC_prediction )\n",
    "print(f\"SVM\\n{SVC_matrix}\\n\\nAccuracy: {SVC_accuracy:.2}\\n\")\n",
    "\n",
    "SVC_score = cross_val_score(estimator = SVC, y = y_train, X = X_train, cv = 10 )\n",
    "print(f\"Score: {SVC_score}\\n\\nMean Score: {SVC_score.mean():.2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phrase: you are amazing\n",
      "\n",
      "Logistic Regression: ['joy']\n",
      "Decision Tree: ['joy']\n",
      "Random Forest: ['joy']\n",
      "Support Vector Machine: ['joy']\n",
      "Naive Bayes: ['surprise']\n",
      "K-Nearest Neighbor: ['joy']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_test_phrase = \"you are amazing\"\n",
    "test_phrase = stemmer.stem(lemmatizer.lemmatize(raw_test_phrase))\n",
    "test_phrase = vectorizer.transform( [test_phrase] ).toarray()\n",
    "\n",
    "print(f'''\n",
    "Phrase: {raw_test_phrase}\n",
    "\n",
    "Logistic Regression: {LR.predict( test_phrase )}\n",
    "Decision Tree: {TREE.predict( test_phrase )}\n",
    "Random Forest: {FOREST.predict( test_phrase )}\n",
    "Support Vector Machine: {SVC.predict( test_phrase )}\n",
    "Naive Bayes: {NB.predict( test_phrase )}\n",
    "K-Nearest Neighbor: {KNN.predict( test_phrase )}\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
